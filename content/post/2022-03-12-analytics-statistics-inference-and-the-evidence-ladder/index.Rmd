---
title: Analytics, Statistics, Inference, and the Evidence Ladder
author: Tim Wilson
date: '2022-03-12'
slug: evidence-ladder
categories:
  - analytics
tags:
  - evidence
  - inference
  - statistics
---

I often feel like my professional development has been a series of iterative cycles around various ideas, techniques, or technologies:

1. Total obliviousness to its existence
2. A vague (but incredibly fuzzy) awareness of it
3. A clearer understanding of it by which I feel like I'm _just_ on the cusp of understanding
4. Multiple years spent in the previous step
5. Finding some explanation or framing that makes it snap _totally_ into focus
6. Realizing that I've simply uncovered a whole bunch of branches of new things that drop me back at step #2

This post lands squarely on #5 above, and it has to do with a combination of words that are similar, but which actually have some pretty profound differences. And I think it matters because they get muddled together in the minds and behaviors of many analysts and marketers. The words:

* Statistic
* Statistics
* Statistical Inference
* Causal Inference

_(Warning: these terms quickly brand out to include quasi-experiments, counterfactuals, natural experiments, randomized controlled trials, A/B testing, field experiments, matched market experiments, and more!)_

I hit #5 when a colleague recently shared a [post from Shopify](https://shopify.engineering/using-quasi-experiments-counterfactuals) that included a diagram showing the "levels of evidence ladder for causal inference methods," which the authors had derived (visually the same...but with altered "rungs" on the ladder) from an [nc233 post](http://nc233.com/2020/04/causal-inference-cheat-sheet-for-data-scientists/).

I'm not going to include either diagram here...because I don't _quite_ follow (agree with?[^1]) some of the ways both posts split up the rungs of the ladder.

[^1]: I'm not looking to start any trouble here. Both posts have 4 levels, but the nc233 has a "top" level that is omitted from the Shopify ladder, and the Shopify post has a "bottom" level that is not part of the nc233 one. This post you're reading now attempts to simplify the ladder to _three_ levels, and if the authors of either post--or anyone, for that matter--would like to eviscerate my simplification, let me go ahead and say it, "You're right. I'm wrong. Please let this footnote suffice as my admission of such rather than insisting that I update this post to add the clarifications you would like to see." (And a gratuitous hat tip to [Benn Stancil](https://twitter.com/bennstancil/) for reminding me, through [his Substack](https://benn.substack.com/), how much fun footnotes can be. And a further hat tip to you, dear reader, for reading the footnotes.)

## A Three-Level Evidence Ladder

Here's the TL;DR:[^2]

[^2]: Clearly, TL;DR is not my forte, given how far into the post we already were before I even tried to declare it.

* A **statistic** is descriptive an (reasonably) deterministic
* **Statistics** are about making predictions about a population from a sample of observations from that population
* **Statistical inference** is about describing relationships between different characteristics within a data set
* **Causal inference** is about describing the impact that one (or multiple) things have (or will have) on something else

Boiling that down to Gilligan's Simplified Evidence Ladder^TM^:

ADD DIAGRAM

## Level 1: Descriptive Statistics

Descriptive statistics is the bread and butter of the marketing analyst, and it lives in the world of _facts_: facts about a given data set. It turns out, what a given data set represents can start to bend your brain a bit: is the data set representing a _population_, or is it representing a _sample from a population_? It...depends on your perspective, I think.

Consider digital analytics data from last month. Descriptive statistics can describe a lot of facts:

* How much traffic came to the site
* The average (mean) traffic per day
* How many orders were placed on the site
* The total revenue of orders placed on the site
* The average order value (AOV) for orders placed on the site
* That maximum daily revenue from the site
* The _minimum_ daily revenue from the site

The list goes on and on and on. And, putting aside the fact that website analytics data is inherently messy to collect, these statistics are all _facts_ about the website's performance last month.

If we are tasked with _reporting the performance of the website_, then last month's data can be considered a _population_, and everything is nice and clean (and, I'm afraid, some will make the case that these are not actually descriptive statistics at all, but, rather, become _parameters_; technically, these folks are likely right, but we're more concerned with the underlying concepts here).

Notice, in this scenario, there is _nothing_ about "prediction" (we have the facts, there is no need to predict anything) or "causation" (we're not asking "why?" We're simply reporting).

What if, though, we consider the _population_ to be "all past, present, and future traffic to the site?" In that case, the data from last month is simply a _sample_ from the population. And, it is decidely _not_ a _random sample_, which means we need to proceed with _extreme caution_ if we want to use last month's data to make claims about this broader population. If there was \$2,000,000 of orders on the website last month, does that mean we can expect \$2,000,000 this month? The answer is a big, fat "maybe." We're moving _beyond_ descriptive statistics and need to move up our evidence ladder if we want to use that data to predict revenue for this month, much less explain what the drivers ("causal factors") of revenue are.

## Level 2: Quasi-Experiments and Counterfactuals

The terminology starts to add a lot syllables at this next level. And, this level is, arguably, the messiest. It's actually so broad and so deep that the posts linked to above split this level into multiple levels!

The basic idea of a counterfactual is that, if we truly had our 'druthers, and if we weren't limited by the current constraints of our ability to manipulate the space-time continuum, any time we had an important decision to make with multiple choices, we would make..._all of them_. We would, simply, split the universe, try a different option in each universe, see which one works out best, and then toss out the other universes and proceed with the best choice.

Say you are considering changing jobs, and you've narrowed it down to three options:

* Stick with your current job
* Accept a new job, for which you have an active offer
* Quit your job, decline the active offer, and see what else comes along once you're rocking that green #OPENTOWORK banner on LinkedIn

Unfortunately, you can't split the universe with today's technology, so you have to pick one of the above. The _other two options are "counterfactuals"_&#8212;the roads not taken.

In this example, you may not have much data available that you think you could crunch in order to make a data-informed decision. It's more an exercise in research and risk-tolerance and qualitative factors.

But, as an analyst, our stakeholders are often swimming in data as they try to make a decision, and our job is to help them use that data to make an informed decision. One way to look at historical data&#8212;say, last month's website traffic&#8212;is through a counterfactual lens:

* We did a bunch of things and observed the (descriptive statistics) results, but
* What _would have happened_ if we had done some different things (counterfactuals!)

If _only_ we could go back to the beginning of last month, split the universe a bunch of different ways, try out a bunch of different things, and then see which worked out the best, we would _crush it_.

But we can't.

So, instead, we're asked to simply _look at the data we do have_ and try to _infer_ relevant relationships. Basically look at the _facts_ and try to establish the..._counterfact(ual)s_. This is...hard:

* Did our increased spend on Google Adwords _cause_ the $2,000,000 in revenue, or
* Did our increased spend on Google Adwords actually have _no incremental effect_ on revenue, or
* Did our increased spend on Google Adwords actually have a _negative incremental effect_ on revenue, but the general seasonal demand for our products offset that effect?

We can't truly _know_. So, instead, we have to try to use statistical modeling to try to make an informed guess as to what happened. Pretty quickly, we'll likely decide we need to look at more data than just last month's&#8212;can we get a year or two's worth, maybe? Then, we set about trying to tease apart and account for as many things as we can think of:

* Maybe we use the months leading up to last month to build a model of what we would have expected to happen last month if we _didn't change a thing_ and then compare the forecast from that model to what actually happened (formally: "estimate the counterfactual of _not_ increasing Google Adwords spend")

* Maybe we find some other metric that has historically trended right along with revenue, but that we would, logically, not expect to have been impacted by a change in Google Adwords spend, and we compare the difference between that metric and revenue _before_ last month to the difference between that metric and revenue when we made the change to our Google Adwords spend (this is "difference-in-difference" or "diff-in-diff")

* Or, maybe we try to use some form of regression to model the apparent relationship between each of our different marketing investments and the result on revenue...and factor in seasonality...and account for other externalities (new entrants into the market, a pandemic, etc.) and then use the results of that model to estimate the contribution of our increased Google Adwords spend last month (this is an oversimplification of media mix modeling that is so extreme that it is bordering on downright offensive)

There are many, many different approaches for this, and they can get quite complicated. Although, they can also start out quite simple! If done well, most of these approaches will make an attempt to quantify how much _uncertainty_ there is in their estimates (we won't be able to eliminate uncertainty until we're able to do that universe-splitting trick described earlier).

If this all sounds quite messy, that's because it is. Unfortunately, it's messiness that, at least in marketing, there has been an aversion to accepting. My take is that this is because we thought that the arrival of "digital" to the world of marketing in the early aughts was going to give us "all the data," and that would then lead us to "the truth." Alas! It only gave us data for _a single universe_: the counterfactuals have continued to be spawned at an infinite pace with 'nary a single piece of data being collected for them!

Except...

## Level 3: Controlled Experimentation

This is the golden rung of the evidence ladder: controlled experimentation is the closest we can get to gathering direct data for our counterfactuals and actually honing in on a cause-and-effect relationship between "things we do" and "the outcomes they produce" ("stimulus-response" is another way to put this; as is "the effect of an intervention").

Let's get out of the way right now that this level goes by _many_ different names. A (likely incomplete) list:[^3]

[^3]: I'm going to ahead and put "natural experiments" on this level, too, but just in a footnote rather than in the main list. In a nutshell, a natural experiment is a case where there just _happened_ to be something that caused randomization of "treatment," so the analyst or researcher wound up having an as-if-it-was-designed-as-a-controlled experiment land in their lap. Natural experiments require a bit of luck, a lot of diligence to ensure they truly meet the requirements, and, often, some extra data crunching because they'll still just be a bit messier than a formally designed experiment. 

* Randomized Controlled Trials
* Field Experiments
* Clinical Trials
* A/B Testing
* Matched Market Testing

These are all the same thing[^4], and they work by simply..._splitting the universe_. Okay, not _exactly_ employing divergence points and space-time continuum hacks into entirely parallel universes but, instead, by divvying up the universe into different groups and treating each bit in a different way. Using the example of a simple A/B test on a website:

* A virtual coin gets flipped every time a new visitor comes to the site
* If the virtual coin comes up "heads," the visitor gets experience A; if it comes up "tails," they get experience B
* We track each visitor and retain whether they were a "heads" or a "tails" user, including what else they did during the remainder of the visit on their site.

After a couple of weeks, we stop the experiment, and we evaluate the group of visits that were "heads" and the group of visits that were the "tails" with respect to our dependent variable(s) (conversion rate, revenue, revenue per visit, etc.).

The difference in the results can then be _causally attributed_ to whatever the difference is between experience A and experience B. This is because we split our universe, so, as a group, the "heads" users and the "tails" users were statistically the same. There is some magic to why all this works that is both intuitively obvious and, for me at least, still mind-blowing to the point that I wound up with a short URL called [http://bit.ly/random-magic](http://bit.ly/random-magic) to link to a simulator that illustrates how this works.

In a nutshell, though: _controlled experimentation_ is both _easier to understand_ AND _the gold standard_ when it comes to getting close to quantifying actual causality!

The downside: well...it takes planning and design and has to actually be executed as an experiment.

## To Summarize...

We took all sorts of simplifying shortcuts here, but the key concepts I _hope_ you leave with are:

* Descriptive statistics are _great_ for simply reporting _facts_ about a set of data.
* Any given data set may be considered to be "the entire population" _or_ a "sample from the population." If it's truly the latter, then we need to not treat it as though it's the former.
* Descriptive statistics are really _not_ a tool for demonstrating causality.
* _Quasi-experimentation_ is one term for a broad and deep set of techniques for using historically observed data to estimate how things would play out in parallel universes (aka, counterfactuals)
* _Controlled experiments_ remove the "quasi" and actually get closer to actually _observing  counterfactuals_.

Pitfalls abound, but, hopefully, this has provided a little bit of clarity when it comes to different approaches and applications of data.





[^4]: If you're determined to tell me that these are _not_ the same thing...please see footnote #1 above.










