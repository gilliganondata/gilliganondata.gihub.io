---
title: Analytics, Statistics, Inference, and the Evidence Ladder
author: Tim Wilson
date: '2022-03-12'
slug: evidence-ladder
categories:
  - analytics
tags:
  - evidence
  - inference
  - statistics
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>“Statistics” is a tricky word. Subtracting a letter or add one of several modifiers, and the meaning of the term (or phrase) changes. And, yet, these different meanings are related to each other. I will boldly claim that there is value in trying to get some clarity as to these differences, as they get muddled together in the minds and behaviors of many analysts and marketers in ways that lead to dangerously misaligned expectations when it comes to what “the data” can deliver in different scenarios</p>
<p>Consider:</p>
<ul>
<li>Statistic</li>
<li>Statistic<em>s</em></li>
<li>Inferential Statistics</li>
<li>Causal Inference (I know…this doesn’t have “statistics,” but chain it to the “inferential” bit above and we’re in the same family of confusion)</li>
</ul>
<p><em>(Warning: these terms quickly brand out to include quasi-experiments, counterfactuals, natural experiments, randomized controlled trials, A/B testing, field experiments, matched market experiments, and more!)</em></p>
<p>A colleague recently shared a <a href="https://shopify.engineering/using-quasi-experiments-counterfactuals">post from Shopify</a> that included a diagram showing the “levels of evidence ladder for causal inference methods,” which the authors had derived (visually the same…but with altered “rungs” on the ladder) from an <a href="http://nc233.com/2020/04/causal-inference-cheat-sheet-for-data-scientists/">nc233 post</a>.</p>
<p>I’m not going to include either diagram here…because I don’t <em>quite</em> follow (agree with?<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>) some of the ways both posts split up the rungs of the ladder.</p>
<div id="a-three-level-evidence-ladder" class="section level2">
<h2>A Three-Level Evidence Ladder</h2>
<p>Here’s the TL;DR:<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<ul>
<li>A <strong>statistic</strong> is descriptive and (reasonably) deterministic</li>
<li><strong>Inferential statistics</strong> are about making predictions about a population from a sample of observations from that population and about describing relationships between different characteristics within a data set</li>
<li><strong>Causal inference</strong> is about describing the impact that one (or multiple) factors have (or will have) on something else</li>
</ul>
<p><em>Causation</em> is where marketers (and analysts) tend to make dangerous leaps between what a given data set can and cannot do. When it comes to <em>making decisions</em>, arguably, <em>causation is key</em>. We want to know, <em>given a set of options when it comes to what we are going to <strong>do</strong>, which one will have the greatest positive <strong>effect</strong>?</em></p>
<p>This doesn’t mean that the <em>only</em> reason we use data is to find causation and make decisions, but it’s one of the main expectations we tend to have of the data.</p>
<p>I absolutely <em>loathe</em> the phrase “actionable insights,” as it gets tossed around with a frequency and abandon that rivals the consumption of frankfurters on Dollar Dog Night at the ballpark. I’m coming to believe that my reaction to the phrase is because I’ve actually had the correct intuition as to what different reports and analyses could (and could <em>not</em>) reasonably deliver. The phrase implies that there is <em>actionability</em> (a decision can be made) based on an <em>insight</em>…which means there is an underlying <em>causal finding</em>.</p>
<p>There <em>is</em> a use of the data that does not have a reliance on causation. That use is simply <em>measuring performance</em>: how much traffic came to the website last month? How many leads have converted to opportunities this quarter? How many customers churned last year?</p>
<p>With a “causation first” focus, we can boil the subject of this post down to Gilligan’s Simplified Causal Evidence Ladder<sup>TM</sup>:</p>
<p><img src="images/evidence-ladder.png" /></p>
</div>
<div id="level-1-descriptive-statistics" class="section level2">
<h2>Level 1: Descriptive Statistics</h2>
<p>Descriptive statistics are the bread and butter of the marketing analyst, and they live in the world of <em>facts</em>: facts about a given data set. It turns out that what a given data set represents can start to bend your brain a bit: is the data set representing a <em>population</em>, or is it representing a <em>sample from a population</em>? It…depends on your perspective!</p>
<p>Consider digital analytics data from last month. Descriptive statistics can describe a lot of facts:</p>
<ul>
<li>How much traffic came to the site</li>
<li>The average (mean) traffic per day</li>
<li>How many orders were placed on the site</li>
<li>The total revenue of orders placed on the site</li>
<li>The average order value (AOV) for orders placed on the site</li>
<li>The maximum daily revenue from the site</li>
<li>The minimum daily revenue from the site</li>
<li>To what extent are traffic to the website and revenue from the website correlated with each other?</li>
</ul>
<p>The list goes on and on and on. And, putting aside the fact that website analytics data is inherently messy to collect, these “statistics” are all simply <em>facts</em> about the website’s performance last month.</p>
<p>If we are tasked with <em>reporting the performance of the website</em>, then last month’s data can be considered a <em>population</em>, and everything is nice and clean (and, I’m afraid, some will make the case that these are not actually descriptive statistics at all, but, rather, become <em>parameters</em>; technically, these folks are likely right, but we’re more concerned with the underlying concepts here).</p>
<p>Notice, in this scenario, there is <em>nothing</em> about “prediction” (we have the facts, there is no need to predict anything) or “causation” (we’re not asking “why?” We’re simply reporting).</p>
<p>What if, though, we consider the <em>population</em> to be “all past, present, and future traffic to the site?” In that case, the data from last month is simply a <em>sample</em> from the population. And, it is decidely <em>not</em> a <em>random sample</em>, which means we need to proceed with <em>extreme caution</em> if we want to use last month’s data to make claims about this broader population. If there was $2,000,000 of orders on the website last month, does that mean we can expect $2,000,000 this month? The answer is a big, fat “maybe.” We’re moving <em>beyond</em> descriptive statistics and need to move up our evidence ladder if we want to use that data to predict revenue for this month, much less explain what the drivers (“causal factors”) of revenue are.</p>
</div>
<div id="level-2-inferential-statistics" class="section level2">
<h2>Level 2: Inferential Statistics</h2>
<p>The terminology starts to add some syllables at this next level. And, this level is, arguably, the messiest. It’s actually so broad and so deep that the posts linked to above split this level into multiple levels!</p>
<p>The basic idea of a counterfactual is that, if we truly had our ’druthers, and if we weren’t limited by the current constraints of our ability to manipulate the space-time continuum, any time we had an important decision to make with multiple choices, we would make…<em>all of them</em>. We would, simply, split the universe, try a different option in each universe, see which one works out best, and then toss out the other universes and proceed with the best choice.</p>
<p>Say you are considering changing jobs, and you’ve narrowed it down to three options:</p>
<ul>
<li>Stick with your current job</li>
<li>Accept a new job, for which you have an active offer</li>
<li>Quit your job, decline the active offer, and see what else comes along once you’re rocking that green #OPENTOWORK banner on LinkedIn</li>
</ul>
<p>Unfortunately, you can’t split the universe with today’s technology, so you have to pick one of the above. The <em>other two options are “counterfactuals”</em>—the roads not taken.</p>
<p>In this example, you may not have much data available that you think you could crunch in order to make a data-informed decision. It’s more an exercise in research and risk-tolerance and qualitative factors.</p>
<p>But, as an analyst, our stakeholders are often swimming in data as they try to make a decision, and our job is to help them use that data to make an informed decision. One way to look at historical data—say, last month’s website traffic—is through a counterfactual lens:</p>
<ul>
<li>We did a bunch of things and observed the (descriptive statistics) results, but</li>
<li>What <em>would have happened</em> if we had done some different things (counterfactuals!)</li>
</ul>
<p>If <em>only</em> we could go back to the beginning of last month, split the universe a bunch of different ways, try out a bunch of different things, and then see which worked out the best, we would <em>crush it</em>.</p>
<p>But we can’t.</p>
<p>So, instead, we’re asked to simply <em>look at the data we do have</em> and try to <em>infer</em> relevant relationships. Basically look at the <em>facts</em> and try to establish the…<em>counterfact(ual)s</em>. This is…hard:</p>
<ul>
<li>Did our increased spend on Google Adwords <em>cause</em> the $2,000,000 in revenue, or</li>
<li>Did our increased spend on Google Adwords actually have <em>no incremental effect</em> on revenue, or</li>
<li>Did our increased spend on Google Adwords actually have a <em>negative incremental effect</em> on revenue, but the general seasonal demand for our products offset that effect?</li>
</ul>
<p>We can’t truly <em>know</em>. So, instead, we have to try to use statistical modeling to try to make an informed guess as to what happened. Pretty quickly, we’ll likely decide we need to look at more data than just last month’s—can we get a year or two’s worth, maybe? Then, we set about trying to tease apart and account for as many things as we can think of:</p>
<ul>
<li><p>Maybe we use the months leading up to last month to build a model of what we would have expected to happen last month if we <em>didn’t change a thing</em> and then compare the forecast from that model to what actually happened (formally: “estimate the counterfactual of <em>not</em> increasing Google Adwords spend”)</p></li>
<li><p>Maybe we find some other metric that has historically trended right along with revenue, but that we would, logically, not expect to have been impacted by a change in Google Adwords spend, and we compare the difference between that metric and revenue <em>before</em> last month to the difference between that metric and revenue when we made the change to our Google Adwords spend (this is “difference-in-difference” or “diff-in-diff”)</p></li>
<li><p>Or, maybe we try to use some form of regression to model the apparent relationship between each of our different marketing investments and the result on revenue…and factor in seasonality…and account for other externalities (new entrants into the market, a pandemic, etc.) and then use the results of that model to estimate the contribution of our increased Google Adwords spend last month (this is an oversimplification of media mix modeling that is so extreme that it is bordering on downright offensive)</p></li>
</ul>
<p>There are many, many different approaches for this, and they can get quite complicated. Although, they can also start out quite simple! If done well, most of these approaches will make an attempt to quantify how much <em>uncertainty</em> there is in their estimates (we won’t be able to eliminate uncertainty until we’re able to do that universe-splitting trick described earlier).</p>
<p>If this all sounds quite messy, that’s because it is. Unfortunately, it’s messiness that, at least in marketing, there has been an aversion to accepting. My take is that this is because we thought that the arrival of “digital” to the world of marketing in the early aughts was going to give us “all the data,” and that would then lead us to “the truth.” Alas! It only gave us data for <em>a single universe</em>: the counterfactuals have continued to be spawned at an infinite pace with ’nary a single piece of data being collected for them!</p>
<p>Except…</p>
</div>
<div id="level-3-experimentation" class="section level2">
<h2>Level 3: Experimentation</h2>
<p>This is the golden rung of the evidence ladder: controlled experimentation is the closest we can get to gathering direct data for our counterfactuals and actually honing in on a cause-and-effect relationship between “things we do” and “the outcomes they produce” (“stimulus-response” is another way to put this; as is “the effect of an intervention”).</p>
<p>Let’s get out of the way right now that this level goes by <em>many</em> different names. A (likely incomplete) list:<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<ul>
<li>Randomized Controlled Trials</li>
<li>Field Experiments</li>
<li>Clinical Trials</li>
<li>A/B Testing</li>
<li>Matched Market Testing</li>
</ul>
<p>These are all the same thing<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>, and they work by simply…<em>splitting the universe</em>. Okay, not <em>exactly</em> employing divergence points and space-time continuum hacks into entirely parallel universes but, instead, by divvying up the universe into different groups and treating each bit in a different way. Using the example of a simple A/B test on a website:</p>
<ul>
<li>A virtual coin gets flipped every time a new visitor comes to the site</li>
<li>If the virtual coin comes up “heads,” the visitor gets experience A; if it comes up “tails,” they get experience B</li>
<li>We track each visitor and retain whether they were a “heads” or a “tails” user, including what else they did during the remainder of the visit on their site.</li>
</ul>
<p>After a couple of weeks, we stop the experiment, and we evaluate the group of visits that were “heads” and the group of visits that were the “tails” with respect to our dependent variable(s) (conversion rate, revenue, revenue per visit, etc.).</p>
<p>The difference in the results can then be <em>causally attributed</em> to whatever the difference is between experience A and experience B. This is because we split our universe, so, as a group, the “heads” users and the “tails” users were statistically the same. There is some magic to why all this works that is both intuitively obvious and, for me at least, still mind-blowing to the point that I wound up with a short URL called <a href="http://bit.ly/random-magic">http://bit.ly/random-magic</a> to link to a simulator that illustrates how this works.</p>
<p>In a nutshell, though: <em>controlled experimentation</em> is both <em>easier to understand</em> AND <em>the gold standard</em> when it comes to getting close to quantifying actual causality!</p>
<p>The downside: well…it takes planning and design and has to actually be executed as an experiment.</p>
</div>
<div id="to-summarize" class="section level2">
<h2>To Summarize…</h2>
<p>We took all sorts of simplifying shortcuts here, but the key concepts I <em>hope</em> you leave with are:</p>
<ul>
<li>Descriptive statistics are <em>great</em> for simply reporting <em>facts</em> about a set of data.</li>
<li>Any given data set may be considered to be “the entire population” <em>or</em> a “sample from the population.” If it’s truly the latter, then we need to not treat it as though it’s the former.</li>
<li>Descriptive statistics are really <em>not</em> a tool for demonstrating causality.</li>
<li><em>Quasi-experimentation</em> is one term for a broad and deep set of techniques for using historically observed data to estimate how things would play out in parallel universes (aka, counterfactuals)</li>
<li><em>Controlled experiments</em> remove the “quasi” and actually get closer to actually <em>observing counterfactuals</em>.</li>
</ul>
<p>Pitfalls abound, but, hopefully, this has provided a little bit of clarity when it comes to different approaches and applications of data.</p>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>I’m not looking to start any trouble here. Both posts have 4 levels, but the nc233 one has a “top” level that is omitted from the Shopify ladder, and the Shopify post has a “bottom” level that is not part of the nc233 one. <em>This</em> post you’re reading now attempts to simplify the ladder to <em>three</em> levels, and if the authors of either post–or anyone, for that matter–would like to eviscerate my simplification, let me go ahead and say it, “You’re right. I’m wrong. Please let this footnote suffice as my admission of such rather than insisting that I update this post to add the clarifications you would like to see.” (And a gratuitous hat tip to <a href="https://twitter.com/bennstancil/">Benn Stancil</a> for reminding me, through <a href="https://benn.substack.com/">his Substack</a>, how much fun footnotes can be. And a further hat tip to you, dear reader, for reading the footnotes.)<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Clearly, TL;DR is not my forte, given how far into the post we already were before I even tried to declare it.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>I’m going to ahead and put “natural experiments” on this level, too, but just in a footnote rather than in the main list. In a nutshell, a natural experiment is a case where there just <em>happened</em> to be something that caused randomization of “treatment,” so the analyst or researcher wound up having an as-if-it-was-designed-as-a-controlled experiment land in their lap. Natural experiments require a bit of luck, a lot of diligence to ensure they truly meet the requirements, and, often, some extra data crunching because they’ll still just be a bit messier than a formally designed experiment.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>If you’re determined to tell me that these are <em>not</em> the same thing…please see footnote #1 above.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
